# -*- coding: utf-8 -*-
"""Sensor_readings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zDnLrheEGyOQzG38mmYtnBLEQXnYFrm3
"""

import pandas as pd
headerList = ['S1','S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S20', 'S21' ,'S22', 'S23', 'S24','class'  ]
df= pd.read_csv('/content/sensor_readings_24 (1).csv', names=headerList,header=0)

df

df.isnull().sum()

df['class'].value_counts()

mapper={'Move-Forward':0,'Sharp-Right-Turn':1,'Slight-Right-Turn':2,'Slight-Left-Turn':3}
df['class']=df['class'].replace(mapper)
df

import matplotlib.pyplot as plt

class_counts = df['class'].value_counts()
class_counts.plot(kind='pie', color='skyblue')
plt.title('Distribution of Classes')
plt.show()

df.plot(kind='density',subplots=True , layout=(5,5) , figsize=(12,12),sharex=False)
plt.show()

from sklearn.preprocessing import StandardScaler,MinMaxScaler

y= df['class']       #split the class column from data
x= df.drop(columns=['class'],axis=1)     # split the 24 features from data

MM = MinMaxScaler()         # Make normalize for features
X_normalized = MM.fit_transform(x)

import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.preprocessing import MinMaxScaler

model=Sequential()
model.add(Dense(64,activation='relu',input_shape=(24,)))
model.add(Dense(32,activation='relu'))
model.add(Dense(16,activation='relu'))

model.add(Dense(4,activation='softmax'))

model.summary()

# create cross validation k-fold

from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import KFold


kf = KFold(n_splits=5, shuffle=True, random_state=42) # Example with 5 folds

acc=[] # to save accuaracy from each fold
all_fold_history = []

for train_index, val_index in kf.split(X_normalized,y):
    X_train_fold, X_val_fold = X_normalized[train_index], X_normalized[val_index]
    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]

    # Train your model on X_train_fold, y_train_fold
    # Evaluate on X_val_fold, y_val_fold

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    early_stopping = EarlyStopping(
        monitor='val_loss',  # Monitor validation loss
        patience=50,          # Number of epochs with no improvement after which training will be stopped
        restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity
    )

    history = model.fit(X_train_fold, y_train_fold, epochs=120, batch_size=64, validation_split=0.2, callbacks=[early_stopping])
    all_fold_history.append(history.history)  # Append the fold's history

    loss, accuracy = model.evaluate(X_val_fold, y_val_fold)
    print(f"Fold accuracy: {accuracy}")
    acc.append(accuracy)
acc=sum(acc)/5

print(f"Average accuracy: {acc}")

import numpy as np
import matplotlib.pyplot as plt


accuracy=[]
val_accuracy=[]
# Assuming all_fold_history is a list of dictionaries containing history for each fold
for fold_hist in all_fold_history:
  accuracy.append(np.mean(fold_hist['accuracy']))
  val_accuracy.append(np.mean(fold_hist['val_accuracy']))


# draw avg training and validation accuracy for each fold

plt.plot(accuracy,label='Training Accuracy')
plt.plot(val_accuracy,label='Validation Accuracy')
plt.legend()
plt.xlabel('folds')
plt.ylabel('Accuracy')
plt.title('Average Accuracy across Folds')
plt.show()